{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0022f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "df = pd.read_csv('siCoData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a932edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Ein: 0.051814 at 755 iterations out of 1000\n",
      "w1 (input to hidden):\n",
      "[[-3.46286813 -0.97876014]\n",
      " [ 6.84086611  1.96227013]]\n",
      "w2 (hidden to output):\n",
      "[[-0.21596718]\n",
      " [ 1.55272521]\n",
      " [-1.32282178]]\n"
     ]
    }
   ],
   "source": [
    "# Setting the seed to 42\n",
    "np.random.seed(42)\n",
    "\n",
    "# Reshaping x and y\n",
    "x = df['x'].values.reshape(-1, 1)\n",
    "y = df['y'].values.reshape(-1, 1)\n",
    "\n",
    "# Normalizing the data using MinMaxScaler\n",
    "# DO I RANGE IT FROM -1 to 1??\n",
    "scaler = MinMaxScaler()\n",
    "x = scaler.fit_transform(x) \n",
    "\n",
    "# Adding a column of ones (bias) to incorporate the intercept term\n",
    "# By adding this column of ones, we can have weight\n",
    "x = np.hstack([np.ones((x.shape[0], 1)), x])\n",
    "\n",
    "# Getting the total number of rows\n",
    "total = x.shape[0]\n",
    "\n",
    "# Setting up the Gradient Descent Parameters\n",
    "# Î± aka learning rate; controls the step size\n",
    "learning_rate = 0.1\n",
    "# Setting up a convergence threshold \n",
    "tolerance = 0.0001\n",
    "# Setting up max iterations\n",
    "max_iterations = 1000\n",
    "\n",
    "# Setting up the input, hidden, and output layer\n",
    "# The input layer has 2 features (the bias and x)\n",
    "input_layer = x.shape[1]\n",
    "# Trying 2 neurons for the hidden layer (tanh)\n",
    "hidden_layer = 2\n",
    "# The output layer has only one output (linear)\n",
    "output_layer = 1\n",
    "\n",
    "# Initializing the weights randomly\n",
    "# Weights from input to hidden \n",
    "w1 = np.random.rand(input_layer, hidden_layer)\n",
    "# Weights from hidden to output; add the bias from the hidden layer (hidden_layer + 1)\n",
    "w2 = np.random.rand(hidden_layer + 1, output_layer)\n",
    "# Initializing to get the minimum Ein; currently sets it to a high value so it can get replaced later\n",
    "min_ein = float('inf')\n",
    "\n",
    "# Applying backpropagation algorithm (SGD)\n",
    "# Looping through the max iterations\n",
    "for i in range(max_iterations):\n",
    "    # Initializing the total error to 0\n",
    "    total_error = 0\n",
    "    \n",
    "    # DO WE NEED TO SHUFFLE OR USE INDICES? \n",
    "    #indices = np.arange(total)\n",
    "    #np.random.shuffle(indices)\n",
    "    \n",
    "    # Looping through each training point\n",
    "    for j in range(total):\n",
    "    #for j in indices:\n",
    "        # Getting the x and y of that training point\n",
    "        xj = x[j].reshape(1, -1)\n",
    "        yj = y[j]\n",
    "        \n",
    "        # Doing all forward calculations (s, theta)\n",
    "        # w^T @ x but since numpy uses row-wise data, x @ W\n",
    "        s1 = np.dot(xj, w1)\n",
    "        theta_s1 = np.tanh(s1)\n",
    "        # Adding the bias in\n",
    "        theta_s1_bias = np.hstack([np.ones((1, 1)), theta_s1])\n",
    "        s2 = np.dot(theta_s1_bias, w2)\n",
    "\n",
    "        # Computing the squared error since we have a linear output\n",
    "        error = (s2 - yj)\n",
    "        # Adding it to the total error\n",
    "        total_error += error**2\n",
    "\n",
    "        # Doing all backward calculations \n",
    "        # For the output layer (linear)\n",
    "        # s^2 = w^2 * theta(s^1)\n",
    "        # de/ds^2 = 2*(error) * ds^2/dw^2 = theta(s^1)\n",
    "        # theta_s1_bias.T because we need to include the bias since this is the input for the output layer and we need\n",
    "        # to calculate the gradient for all output weights\n",
    "        dw2 = np.dot(theta_s1_bias.T, 2 * error) \n",
    "        \n",
    "        # For the hidden layer \n",
    "        # 2*(error) * w^2j * (1 - theta^2(s^1))\n",
    "        # w2[1:].T represents each neuron in the hidden layer; don't need the bias\n",
    "        delta_hidden = np.dot(2 * error, w2[1:].T) * (1 - np.tanh(s1)**2)\n",
    "\n",
    "        # For the input layer\n",
    "        # 2*(error) * w^2j * (1 - theta^2(s^1)) * xi; don't need the bias\n",
    "        dw1 = np.dot(xj.T, delta_hidden)\n",
    "\n",
    "        # Updating the weights\n",
    "        w2 -= learning_rate * dw2\n",
    "        w1 -= learning_rate * dw1\n",
    "\n",
    "    # Calculating the in-sample error for each iteration\n",
    "    # ein = summation of the errors / N\n",
    "    ein = total_error / total\n",
    "\n",
    "    # Getting the minimum ein and its iteration\n",
    "    if ein < min_ein:\n",
    "        min_ein = ein\n",
    "        iteration = i + 1\n",
    "    \n",
    "    # Checking for convergence\n",
    "    if ein < tolerance:\n",
    "        break\n",
    "\n",
    "print(f'Minimum Ein: {min_ein.item():.6f} at {iteration} iterations out of {max_iterations}')\n",
    "print('w1 (input to hidden):')\n",
    "print(w1)\n",
    "print('w2 (hidden to output):')\n",
    "print(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f5984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('siCoData.csv')\n",
    "X = data['x'].values.reshape(-1, 1)\n",
    "y = data['y'].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize input\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Add bias to input\n",
    "X_with_bias = np.hstack([np.ones((X.shape[0], 1)), X])  # shape (n_samples, 2)\n",
    "\n",
    "# Define activation and derivative\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "# Initialize weights\n",
    "np.random.seed(42)\n",
    "W1 = np.random.rand(X_with_bias.shape[1], 2)     # (2, 2)\n",
    "W2 = np.random.rand(2 + 1, 1)                    # (3, 1)                   # (3, 1) for 2 hidden + bias\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "max_iterate = 1000\n",
    "tolerance = 0.0001\n",
    "\n",
    "min_Ein = float('inf')\n",
    "best_iter = 0\n",
    "best_W1 = None\n",
    "best_W2 = None\n",
    "\n",
    "for iterate in range(max_iterate):\n",
    "    total_error = 0\n",
    "\n",
    "    for i in range(len(X_with_bias)):\n",
    "        x_i = X_with_bias[i].reshape(1, -1)  # (1, 2)\n",
    "        y_i = y[i]                           # scalar value\n",
    "\n",
    "        # Forward pass\n",
    "        s1 = np.dot(x_i, W1)                # (1, 2)\n",
    "        theta_s1 = tanh(s1)                 # (1, 2)\n",
    "\n",
    "        # Add bias to hidden layer\n",
    "        theta_s1_bias = np.hstack([np.ones((1, 1)), theta_s1])  # (1, 3)\n",
    "\n",
    "        s2 = np.dot(theta_s1_bias, W2)      # (1, 1)\n",
    "        y_hat = s2\n",
    "\n",
    "        error = y_hat - y_i\n",
    "        total_error += (error ** 2)[0]\n",
    "\n",
    "        # Backward pass\n",
    "        dW2 = np.dot(theta_s1_bias.T, 2 * error)                # (3, 1)\n",
    "        delta_hidden = np.dot(2 * error, W2[1:].T) * tanh_derivative(s1)  # (1, 2)\n",
    "        dW1 = np.dot(x_i.T, delta_hidden)                       # (2, 2)\n",
    "\n",
    "        # Update weights\n",
    "        W2 -= learning_rate * dW2\n",
    "        W1 -= learning_rate * dW1\n",
    "\n",
    "    Ein = total_error / len(X_with_bias)\n",
    "\n",
    "    if Ein < min_Ein:\n",
    "        min_Ein = Ein\n",
    "        best_iter = iterate\n",
    "        best_W1 = W1\n",
    "        best_W2 = W2\n",
    "\n",
    "    if Ein < tolerance:\n",
    "        break\n",
    "\n",
    "print(f\"Minimum Ein: {min_Ein}, at iteration: {best_iter}\")\n",
    "print(\"Best W1 (input to hidden):\")\n",
    "print(best_W1)\n",
    "print(\"Best W2 (hidden to output):\")\n",
    "print(best_W2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
